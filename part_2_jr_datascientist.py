# -*- coding: utf-8 -*-
"""Part_2_JrDataScientist.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v4AanxpoM4Fs4gbKZOU238Yq3X3S8ZCP
"""

from google.colab import drive
drive.mount('/content/drive')

# Import necessary libraries
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler
from tensorflow.keras.layers import BatchNormalization
from sklearn.metrics import roc_auc_score
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
import joblib

train_data = pd.read_csv('/content/drive/MyDrive/EvaluationFile/train_data_evaluation_part_2.csv')
test_data = pd.read_csv('/content/drive/MyDrive/EvaluationFile/test_data_evaluation_part2.csv')

train_data.head()

train_data.tail()

train_data.shape

train_data.info()

train_data.describe()

train_data.nunique()

train_data.isnull().sum()

# Handle missing values
for df in [train_data, test_data]:
    df['Age'] = df['Age'].fillna(df['Age'].mean())
# Remove invalid ages
train_data = train_data[train_data['Age'] >= 0]
test_data = test_data[test_data['Age'] >= 0]

train_data.isnull().sum()

# Calculate correlation coefficient
correlation = train_data['PersonsNights'].corr(train_data['RoomNights'])
print(f"Correlation between PersonsNights and RoomNights: {correlation:.2f}")

# Visualize the relationship using a scatter plot
plt.figure(figsize=(8, 6))
sns.scatterplot(x=train_data['PersonsNights'], y=train_data['RoomNights'])
plt.title('PersonsNights vs RoomNights')
plt.xlabel('PersonsNights')
plt.ylabel('RoomNights')
plt.show()

# Check correlation between PersonsNights and RoomNights
correlation = train_data[['PersonsNights', 'RoomNights']].corr().iloc[0, 1]
print(f"Correlation between PersonsNights and RoomNights: {correlation:.2f}")

data = train_data["Age"]

# Creating boxplot
sns.boxplot(data)

# Adding title and labels
plt.title('Boxplot Example')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')

# Show plot
plt.show()

data = train_data["PersonsNights"]

# Creating boxplot
sns.boxplot(data)

# Adding title and labels
plt.title('Boxplot Example')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')

# Show plot
plt.show()

for df in [train_data, test_data]:
  if 'PersonsNights' in df.columns:
        df['PersonsNights'] = df['PersonsNights'].fillna(df['PersonsNights'].median())

#Drop irrelevant columns
columns_to_drop = [
    "ID", "Nationality", "Unnamed: 0", "DaysSinceCreation", "DaysSinceFirstStay",
    "MarketSegment", "RoomNights", "SRHighFloor", "SRLowFloor", "SRAccessibleRoom", "SRMediumFloor",
    "SRBathtub", "SRShower", "SRCrib", "SRKingSizeBed", "SRTwinBed", "SRNearElevator", "SRAwayFromElevator",
    "SRNoAlcoholInMiniBar", "SRQuietRoom"
]
train_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')
test_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')

train_data.info()

# Set figure size
plt.figure(figsize=(18, 6))

# Visualize distribution of the target variable
ax = sns.countplot(data=train_data, x='BookingsCheckedIn')
plt.title('Distribution of Checked-In Status')

# Add labels to each bar
for bar in ax.patches:
    count = int(bar.get_height())  # Get the height of each bar (the count)
    x_position = bar.get_x() + bar.get_width() / 2  # Center the label on the bar
    ax.text(x_position, count, f'{count}', ha='center', va='bottom')

plt.show()

# Analyze and visualize the dataset
def analyze_dataset(data, title):
    print("Feature Distributions")
    for column in data.select_dtypes(include=['float64', 'int64','object']).columns:
        plt.figure(figsize=(10, 6))
        sns.histplot(data[column], kde=True, bins=30, color='Red')
        plt.title(f"Distribution of {column} ({title})")
        plt.show()

# Analyze the train and test datasets
analyze_dataset(train_data, "Training Data")

#Handle outliers using IQR
def cap_outliers(series, multiplier=1.5):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - (multiplier * IQR)
    upper_bound = Q3 + (multiplier * IQR)
    return np.clip(series, lower_bound, upper_bound)

outlier_columns = ['Age', 'PersonsNights', 'AverageLeadTime', 'LodgingRevenue', 'OtherRevenue', 'DaysSinceLastStay']
for col in outlier_columns:
    if col in train_data.columns:
        train_data[col] = cap_outliers(train_data[col])
    if col in test_data.columns:
        test_data[col] = cap_outliers(test_data[col])

# Create new features
for df in [train_data, test_data]:
    if all(col in df.columns for col in ['PersonsNights', 'LodgingRevenue', 'OtherRevenue']):
        df['RevenuePerNight'] = (
            (df['LodgingRevenue'] + df['OtherRevenue']) / df['PersonsNights']
        ).replace([np.inf, -np.inf], 0).fillna(0)

# Scale numerical features
scaler = MinMaxScaler()
numerical_columns = ['Age', 'PersonsNights', 'AverageLeadTime', 'LodgingRevenue', 'OtherRevenue', 'DaysSinceLastStay', 'RevenuePerNight']
train_data[numerical_columns] = scaler.fit_transform(train_data[numerical_columns])
test_data[numerical_columns] = scaler.transform(test_data[numerical_columns])

# Save the scaler for later use
joblib.dump(scaler, 'scaler.pkl')

# Create derived features
for df in [train_data, test_data]:
    df['TotalBookings'] = df['BookingsCanceled'] + df['BookingsNoShowed'] + df['BookingsCheckedIn']
    df['CheckInRate'] = df['BookingsCheckedIn'] / df['TotalBookings']
    df['WillCheckIn'] = df['CheckInRate'].apply(lambda x: 1 if x >= 0.5 else 0)
    df.drop(columns=['BookingsCanceled', 'BookingsNoShowed', 'BookingsCheckedIn', 'TotalBookings', 'CheckInRate'], inplace=True)

train_data.head(5)

train_data.dtypes

# One-hot encode categorical features
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
encoded_features_train = pd.DataFrame(
    encoder.fit_transform(train_data[['DistributionChannel']]),
    columns=encoder.get_feature_names_out(['DistributionChannel']),
    index=train_data.index
)
train_data = pd.concat([train_data.drop(columns=['DistributionChannel']), encoded_features_train], axis=1)

encoded_features_test = pd.DataFrame(
    encoder.transform(test_data[['DistributionChannel']]),
    columns=encoder.get_feature_names_out(['DistributionChannel']),
    index=test_data.index
)
test_data = pd.concat([test_data.drop(columns=['DistributionChannel']), encoded_features_test], axis=1)

# Save the encoder for later use
joblib.dump(encoder, 'encoder.pkl')

train_data.dtypes

# Analyze and visualize the dataset
def analyze_dataset(data, title):
    print("Clean Distributions")
    for column in data.select_dtypes(include=['float64', 'int64']).columns:
        plt.figure(figsize=(10, 6))
        sns.histplot(data[column], kde=True, bins=30, color='Green')
        plt.title(f"Distribution of {column} ({title})")
        plt.show()

# Analyze the train and test datasets
analyze_dataset(train_data, "Training Data")

# Separate features and target
X_train = train_data.drop(columns=['WillCheckIn'])
y_train = train_data['WillCheckIn']
X_test = test_data.drop(columns=['WillCheckIn'])
y_test = test_data['WillCheckIn']

# Handle class imbalance
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
class_weights_dict = dict(enumerate(class_weights))

# Standardize the data
scaler_standard = StandardScaler()
X_train = scaler_standard.fit_transform(X_train)
X_test = scaler_standard.transform(X_test)

# Save the standard scaler
joblib.dump(scaler_standard, 'standard_scaler.pkl')

# Convert DataFrames to NumPy arrays
X_train = np.array(X_train)
X_test = np.array(X_test)
y_train = np.array(y_train)
y_test = np.array(y_test)

# Build the neural network model
model = Sequential([
    Input(shape=(X_train.shape[1],)),  # Define input shape explicitly
    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
    BatchNormalization(),
    Dropout(0.3),  # Balanced dropout for regularization
    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),
    BatchNormalization(),
    Dropout(0.3),
    Dense(32, activation='relu', kernel_regularizer=l2(0.01)),
    BatchNormalization(),
    Dropout(0.3),
    Dense(1, activation='sigmoid')  # Sigmoid for binary classification
])

# Compile the model with a tuned optimizer
optimizer = Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Early stopping to avoid overfitting
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=5,  # Reduced patience for early stopping
    restore_best_weights=True,
    verbose=1
)

# Early stopping and learning rate scheduler
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)
def scheduler(epoch, lr):
    if epoch > 10:
        return lr * 0.5
    return lr
lr_scheduler = LearningRateScheduler(scheduler, verbose=1)

# Train the model
history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    class_weight=class_weights_dict,
    callbacks=[early_stopping, lr_scheduler],
    verbose=1
)


# Plot training and validation metrics
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy')
plt.legend()
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss')
plt.legend()
plt.show()

# Save the trained model
model.save('my_model.keras')

# Evaluate the model on test data
y_test_pred = (model.predict(X_test) > 0.5).astype(int)
test_mae = mean_absolute_error(y_test, y_test_pred)
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
test_accuracy = accuracy_score(y_test, y_test_pred)

print(f"Test MAE: {test_mae:.4f}")
print(f"Test RMSE: {test_rmse:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")